import streamlit as st
from video_card import video_card
from card import card

# ================== PAGE: REINFORCEMENT LEARNING ==================
def reinforcement_learning_page():
    st.title("ğŸ® Reinforcement Learning â€“ Dáº¡y AI tá»± há»c nhÆ° con ngÆ°á»i")
    st.info("Tá»« khÃ¡i niá»‡m cÆ¡ báº£n Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n nÃ¢ng cao - dáº¡y AI chÆ¡i game vÃ  giáº£i quyáº¿t bÃ i toÃ¡n phá»©c táº¡p")

    # ================== CHÆ¯Æ NG 1 ==================
    st.markdown("---")
    st.header("ğŸŒŸ ChÆ°Æ¡ng 1: KhÃ¡i Niá»‡m CÆ¡ Báº£n Reinforcement Learning")
    st.info("ğŸ¯ Má»¥c tiÃªu: Hiá»ƒu báº£n cháº¥t RL, Markov Decision Process, cÃ¡c thÃ nh pháº§n cá»‘t lÃµi cá»§a há»‡ thá»‘ng RL")

    with st.expander("Xem chi tiáº¿t: "):
        # BÃ i há»c
        col1, _, col2 = st.columns([8, 1, 8])
        with col1:
            card("BÃ i 1: Reinforcement Learning lÃ  gÃ¬?",
                 "Há»c qua tÆ°Æ¡ng tÃ¡c vá»›i mÃ´i trÆ°á»ng. Agent, Environment, Action, State, Reward. So sÃ¡nh vá»›i Supervised vÃ  Unsupervised Learning.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: NguyÃªn lÃ½ cÆ¡ báº£n cá»§a RL",
                 "Trial-and-error learning. Exploration vs Exploitation dilemma. Reward hypothesis. VÃ­ dá»¥ thá»±c táº¿: dáº¡y tráº» Ä‘i xe Ä‘áº¡p, AlphaGo.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Markov Decision Process (MDP)",
                 "States, Actions, Rewards, Transitions. Markov Property: 'Future chá»‰ phá»¥ thuá»™c Present'. Finite vs Infinite MDP examples.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Tá»•ng thÆ°á»Ÿng vÃ  Discount Factor",
                 "Cumulative reward, Return. Discount factor Î³: táº¡i sao cáº§n discount? Finite vs Infinite horizon. Present value concept.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
        with col2:
            card("BÃ i 5: Policy (Chiáº¿n lÆ°á»£c)",
                 "Deterministic vs Stochastic policy. Ï€(a|s): xÃ¡c suáº¥t chá»n action a á»Ÿ state s. Optimal policy Ï€*.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Value Functions",
                 "State-value function V(s), Action-value function Q(s,a). Bellman Equation trá»±c quan. Recursive relationship.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Bellman Equations",
                 "Bellman Expectation Equation, Bellman Optimality Equation. Relationship giá»¯a V* vÃ  Q*. Fixed point theorem.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: RL Problem Types",
                 "Prediction vs Control problems. Model-free vs Model-based. On-policy vs Off-policy. Episodic vs Continuing tasks.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        card("Lab 1: GridWorld Environment",
             "Táº¡o simple grid world, define MDP components, visualize policy vÃ  value functions, implement random policy evaluation.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Lab 2: FrozenLake Environment",
             "OpenAI Gym FrozenLake, khÃ¡m phÃ¡ environment interface, visualize MDP, analyze optimal policy manually.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 2 ==================
    st.markdown("---")
    st.header("ğŸ”„ ChÆ°Æ¡ng 2: Dynamic Programming Methods")
    st.info("ğŸ¯ Má»¥c tiÃªu: Giáº£i MDP khi biáº¿t model hoÃ n chá»‰nh vá»›i Policy Iteration vÃ  Value Iteration")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Láº­p trÃ¬nh Ä‘á»™ng trong RL",
                 "Dynamic Programming assumptions: perfect model, finite MDP. Bootstrapping concept. Iterative improvement methods.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Policy Evaluation",
                 "Iterative Policy Evaluation algorithm. Convergence proof. Synchronous vs Asynchronous updates. Implementation tricks.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Policy Improvement",
                 "Policy Improvement Theorem. Greedy policy construction. Policy Improvement Theorem proof intuition.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Policy Iteration Algorithm",
                 "Káº¿t há»£p Policy Evaluation + Policy Improvement. Convergence guarantee. Step-by-step walkthrough vá»›i GridWorld example.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: Value Iteration Algorithm",
                 "Direct search for optimal value function. Value Iteration theorem. So sÃ¡nh vá»›i Policy Iteration: speed vs simplicity.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Asynchronous DP Methods",
                 "In-place value iteration. Prioritized sweeping. Gauss-Seidel vs Jacobi methods. Real-time dynamic programming.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Generalized Policy Iteration",
                 "Framework cho most RL algorithms. Interaction giá»¯a evaluation vÃ  improvement. Convergence intuition.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: Limitations cá»§a DP",
                 "Curse of dimensionality. Perfect model assumption. Computational complexity. Transition sang model-free methods.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Projects
        card("Project 1: Solving GridWorld",
             "Implement Policy Iteration vÃ  Value Iteration tá»« scratch, visualize convergence, compare algorithms, analyze optimal policies.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 2: Inventory Management",
             "DP solution cho inventory control problem, model uncertainty demand, optimize reorder policies, visualize value function.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 3 ==================
    st.markdown("---")
    st.header("ğŸ² ChÆ°Æ¡ng 3: Monte Carlo Methods")
    st.info("ğŸ¯ Má»¥c tiÃªu: Há»c tá»« experience episodes khi khÃ´ng biáº¿t model, First-visit vÃ  Every-visit MC")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Monte Carlo trong RL",
                 "Há»c tá»« complete episodes. Model-free learning. Law of Large Numbers. Unbiased estimates tá»« sampling.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Monte Carlo Policy Evaluation",
                 "Estimate V(s) tá»« returns. First-visit vs Every-visit MC. Incremental mean updates. Convergence properties.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Monte Carlo Estimation of Action Values",
                 "Estimate Q(s,a) thay vÃ¬ V(s). Exploring starts assumption. Importance cá»§a action-value functions cho control.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Monte Carlo Control",
                 "MC version cá»§a Generalized Policy Iteration. Policy improvement step. Exploring Starts method.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: Îµ-greedy Policies",
                 "Maintaining exploration trong control. Îµ-greedy policy improvement. Convergence vá»›i diminishing Îµ.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Off-policy Prediction",
                 "Importance sampling. Ordinary vs Weighted importance sampling. Target policy vs Behavior policy.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Off-policy Monte Carlo Control",
                 "Off-policy MC control algorithm. Incremental implementation. Variance issues vá»›i importance sampling.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: MC vs DP Comparison",
                 "Advantages: model-free, focus on important states. Disadvantages: high variance, episode requirement.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Projects
        card("Project 1: Blackjack vá»›i MC",
             "Classic Blackjack environment, implement First-visit MC prediction, estimate state values, visualize learned policy.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 2: MC Control cho GridWorld",
             "Compare MC Control vs Policy Iteration, analyze convergence rates, study effect of exploration parameters.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 4 ==================
    st.markdown("---")
    st.header("âš¡ ChÆ°Æ¡ng 4: Temporal Difference Learning")
    st.info("ğŸ¯ Má»¥c tiÃªu: Combine MC vÃ  DP advantages vá»›i TD learning, Q-Learning vÃ  SARSA")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Temporal Difference Learning",
                 "Bootstrap + Sampling. TD(0) algorithm. TD error concept. So sÃ¡nh vá»›i MC vÃ  DP methods.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: TD Prediction Algorithm",
                 "TD(0) for policy evaluation. Step-size parameter Î±. Online learning vs batch methods. Convergence properties.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Q-Learning tá»« A-Z",
                 "Off-policy TD control. Q-Learning algorithm derivation. Optimal action-value function learning. Implementation details.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Q-Learning Implementation",
                 "Q-table representation. Action selection strategies. Learning rate schedules. Exploration decay strategies.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: SARSA Algorithm",
                 "On-policy TD control. SARSA vs Q-Learning differences. Expected SARSA variant. Policy dependency.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Q-Learning vs SARSA Comparison",
                 "Off-policy vs On-policy learning. 'Tu TiÃªn' analogy: SARSA conservative, Q-Learning aggressive. Cliff Walking example.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: N-step TD Methods",
                 "N-step returns. TD(Î») introduction. Eligibility traces concept. Bias-variance tradeoff trong n-step methods.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: Function Approximation Preview",
                 "Limitations cá»§a tabular methods. Linear function approximation. Neural networks cho value functions. Transition sang Deep RL.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Lab Projects
        card("Lab 1: Q-Learning CartPole",
             "Discretize continuous state space, implement Q-Learning, balance CartPole, hyperparameter tuning, convergence analysis.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Lab 2: SARSA CartPole Implementation",
             "SARSA algorithm cho CartPole, compare vá»›i Q-Learning performance, analyze on-policy vs off-policy behavior.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 5 ==================
    st.markdown("---")
    st.header("ğŸ¤– ChÆ°Æ¡ng 5: Deep Reinforcement Learning")
    st.info("ğŸ¯ Má»¥c tiÃªu: Káº¿t há»£p Deep Learning vá»›i RL - DQN, Policy Gradients, Actor-Critic methods")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Deep Q-Networks (DQN)",
                 "Neural networks cho Q-function approximation. Experience replay buffer. Target network concept. DQN algorithm overview.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: DQN Improvements",
                 "Double DQN, Dueling DQN, Prioritized Experience Replay. Rainbow DQN overview. Addressing overestimation bias.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Policy Gradient Methods",
                 "Direct policy optimization. REINFORCE algorithm. Policy gradient theorem. Baseline Ä‘á»ƒ reduce variance.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Actor-Critic Methods",
                 "Combine value estimation vÃ  policy optimization. Advantage function. A2C (Advantage Actor-Critic) algorithm.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: Proximal Policy Optimization (PPO)",
                 "Trust region methods intuition. Clipped objective function. PPO-Clip algorithm. Why PPO is popular?",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Soft Actor-Critic (SAC)",
                 "Maximum entropy RL framework. Continuous action spaces. Temperature parameter. SAC algorithm components.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Multi-Agent RL",
                 "Independent learners, centralized training/decentralized execution. Nash equilibrium concepts. Cooperation vs competition.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: RL Applications & Future",
                 "Game playing (AlphaGo, OpenAI Five), robotics, autonomous vehicles, recommendation systems. Challenges vÃ  future directions.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Major Projects
        card("Project 1: DQN Flappy Bird",
             "Train DQN agent chÆ¡i Flappy Bird, implement experience replay, target network, hyperparameter tuning, visualize learning process.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 2: PPO Car Racing",
             "Train PPO agent cho CarRacing-v0, continuous action space, CNN feature extraction, reward shaping, performance analysis.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 3: SAC Walker2d",
             "Train SAC agent há»c Ä‘i bá»™ trong Walker2d environment, continuous control, compare vá»›i PPO, locomotion analysis.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        # Final Project
        card("Capstone: Multi-Environment RL Benchmark",
             "So sÃ¡nh multiple algorithms (DQN, PPO, SAC) trÃªn various environments, implement evaluation metrics, create RL agent comparison dashboard.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    st.markdown("---")
    st.success("ğŸ‘‰ HoÃ n thÃ nh Reinforcement Learning track, báº¡n sáº½ hiá»ƒu sÃ¢u vá» RL algorithms, cÃ³ thá»ƒ train AI agents giáº£i quyáº¿t complex tasks vÃ  develop cutting-edge RL applications!")