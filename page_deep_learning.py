import streamlit as st
from video_card import video_card
from card import card

# ================== PAGE: DEEP LEARNING ==================
def deep_learning_page():
    st.title("ğŸ§  Deep Learning â€“ Tá»« Neural Network Ä‘áº¿n AI hiá»‡n Ä‘áº¡i")
    st.info("Tá»« perceptron Ä‘Æ¡n giáº£n Ä‘áº¿n Transformer - ná»n táº£ng cá»§a ChatGPT vÃ  cÃ¡c AI model tiÃªn tiáº¿n")

    # ================== CHÆ¯Æ NG 1 ==================
    st.markdown("---")
    st.header("ğŸ”¥ ChÆ°Æ¡ng 1: Neural Networks CÆ¡ Báº£n")
    st.info("ğŸ¯ Má»¥c tiÃªu: Hiá»ƒu báº£n cháº¥t neural network, tá»« perceptron Ä‘áº¿n MLP, lÃ m chá»§ backpropagation")

    with st.expander("Xem chi tiáº¿t: "):
        # BÃ i há»c
        col1, _, col2 = st.columns([8, 1, 8])
        with col1:
            card("BÃ i 1: Tá»« nÃ£o bá»™ Ä‘áº¿n AI",
                 "Neurons sinh há»c vs artificial neurons. Perceptron Ä‘Æ¡n giáº£n: inputs, weights, bias, activation. Táº¡i sao cáº§n nhiá»u layers?",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Multi-layer Perceptron (MLP)",
                 "Architecture: Input â†’ Hidden â†’ Output layers. Forward pass step by step. Activation functions: Sigmoid, ReLU, Tanh, Softmax.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Backpropagation - trá»±c quan siÃªu dá»… hiá»ƒu",
                 "Thuáº­t toÃ¡n 'há»c' cá»§a neural network. Chain rule in action. Gradient flow qua cÃ¡c layers. Visualize vá»›i computational graph.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Thá»±c hÃ nh vá»›i TensorFlow/Keras",
                 "XÃ¢y dá»±ng MLP Ä‘áº§u tiÃªn: Sequential model, Dense layers, compile, fit. Hyperparameters: learning_rate, batch_size, epochs.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
        with col2:
            card("BÃ i 5: Loss Functions cho Deep Learning",
                 "Binary crossentropy, Categorical crossentropy, Sparse categorical crossentropy. MSE cho regression. Custom loss functions.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Optimizers nÃ¢ng cao",
                 "SGD, Adam, RMSprop, AdaGrad. Learning rate scheduling. Adaptive learning rates. So sÃ¡nh convergence speed.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Regularization trong Deep Learning",
                 "Overfitting trong neural networks. Dropout, Batch Normalization, L1/L2 regularization. Early stopping vá»›i callbacks.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: Debugging Neural Networks",
                 "Learning curves, gradient vanishing/exploding. Weight initialization. Monitoring training process vá»›i TensorBoard.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        card("Mini Project 1: PhÃ¢n loáº¡i chá»¯ sá»‘ viáº¿t tay",
             "Dataset MNIST: xÃ¢y dá»±ng MLP tá»« scratch, so sÃ¡nh activation functions, visualize learned weights, achieve >95% accuracy.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Mini Project 2: Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vá»›i Neural Network",
             "Dataset Boston Housing: MLP cho regression, feature scaling, hyperparameter tuning, so sÃ¡nh vá»›i linear regression.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 2 ==================
    st.markdown("---")
    st.header("ğŸ–¼ï¸ ChÆ°Æ¡ng 2: Convolutional Neural Networks (CNN)")
    st.info("ğŸ¯ Má»¥c tiÃªu: LÃ m chá»§ CNN cho Computer Vision - tá»« edge detection Ä‘áº¿n image classification nÃ¢ng cao")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Táº¡i sao MLP khÃ´ng phÃ¹ há»£p vá»›i áº£nh?",
                 "Váº¥n Ä‘á» vá»›i high-dimensional images. Spatial structure vÃ  translation invariance. Convolution operation trá»±c quan.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Convolution & Feature Maps",
                 "Kernels/Filters há»c detect patterns. Stride, padding, feature maps. Visualize convolution step-by-step vá»›i animations.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Pooling & CNN Architecture",
                 "Max pooling, Average pooling. CNN = Conv â†’ Pool â†’ Conv â†’ Pool â†’ Flatten â†’ Dense. Receptive field concept.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: CNN vá»›i Keras",
                 "Conv2D, MaxPool2D, Flatten layers. Input shape cho images. Data augmentation vá»›i ImageDataGenerator. Model summary visualization.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: Advanced CNN Techniques",
                 "Batch Normalization trong CNN. Dropout cho overfitting. Global Average Pooling thay cho Dense layers cuá»‘i.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Transfer Learning",
                 "Sá»­ dá»¥ng pre-trained models: VGG16, ResNet50, MobileNet. Feature extraction vs Fine-tuning. Freezing layers.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: CNN Architectures ná»•i tiáº¿ng",
                 "LeNet, AlexNet, VGG, ResNet, Inception. Residual connections giáº£i quyáº¿t vanishing gradient. Architecture evolution.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: Object Detection cÆ¡ báº£n",
                 "Image classification vs Object detection vs Segmentation. YOLO concept. Bounding boxes vÃ  confidence scores.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Projects
        card("Project 1: Image Classification",
             "Dataset CIFAR-10: xÃ¢y dá»±ng CNN tá»« Ä‘áº§u, data augmentation, visualize feature maps, achieve >80% accuracy, so sÃ¡nh vá»›i pre-trained model.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 2: Medical Image Analysis",
             "Dataset X-ray pneumonia: transfer learning vá»›i ResNet50, fine-tuning, class imbalance handling, medical AI ethics considerations.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 3 ==================
    st.markdown("---")
    st.header("ğŸ”„ ChÆ°Æ¡ng 3: Recurrent Neural Networks (RNN)")
    st.info("ğŸ¯ Má»¥c tiÃªu: Xá»­ lÃ½ dá»¯ liá»‡u sequence vá»›i RNN, LSTM, GRU - á»©ng dá»¥ng NLP vÃ  time series")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Sequential Data & RNN Motivation",
                 "Dá»¯ liá»‡u chuá»—i: text, time series, speech. Táº¡i sao CNN/MLP khÃ´ng phÃ¹ há»£p? RNN architecture vá»›i hidden state memory.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Vanilla RNN",
                 "RNN cell: input + previous hidden state â†’ new hidden state. Many-to-one, one-to-many, many-to-many architectures. Vanishing gradient problem.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Long Short-Term Memory (LSTM)",
                 "Giáº£i quyáº¿t vanishing gradient. Forget gate, Input gate, Output gate. Cell state vs Hidden state. LSTM intuition vá»›i analogies.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Gated Recurrent Unit (GRU)",
                 "LSTM Ä‘Æ¡n giáº£n hÃ³a. Reset gate, Update gate. So sÃ¡nh LSTM vs GRU: performance, computational cost.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: RNN vá»›i Keras",
                 "SimpleRNN, LSTM, GRU layers. return_sequences parameter. Bidirectional RNNs. Stacking RNN layers.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Text Preprocessing cho RNN",
                 "Tokenization, Vocabulary building. Padding sequences. Word embeddings: Word2Vec concept, Embedding layer trong Keras.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Sequence-to-Sequence Models",
                 "Encoder-Decoder architecture. Language translation intuition. Attention mechanism preview (chuáº©n bá»‹ cho Transformer).",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: Time Series Forecasting",
                 "RNN cho dá»± bÃ¡o. Sliding window approach. Multi-step prediction. Evaluation metrics: MAE, RMSE cho time series.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Projects
        card("Project 1: Sentiment Analysis",
             "Dataset movie reviews: text preprocessing, word embeddings, LSTM classifier, compare vá»›i traditional ML, interpretability vá»›i attention weights.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 2: Stock Price Prediction",
             "Dataset historical stock prices: feature engineering, LSTM time series model, walk-forward validation, risk disclaimer vÃ  limitations.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 4 ==================
    st.markdown("---")
    st.header("âš¡ ChÆ°Æ¡ng 4: Transformer Architecture")
    st.info("ğŸ¯ Má»¥c tiÃªu: Hiá»ƒu kiáº¿n trÃºc Transformer - ná»n táº£ng cá»§a ChatGPT, BERT, GPT vÃ  cÃ¡c LLM hiá»‡n Ä‘áº¡i")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Attention is All You Need",
                 "Váº¥n Ä‘á» cá»§a RNN: sequential processing, long dependencies. Self-attention mechanism. Query, Key, Value concept trá»±c quan.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Self-Attention Mechanism",
                 "Scaled Dot-Product Attention step-by-step. Attention weights visualization. Multi-Head Attention: nhiá»u 'gÃ³c nhÃ¬n' khÃ¡c nhau.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Transformer Block Architecture",
                 "Encoder stack: Multi-Head Attention + Feed-Forward + Residual connections + Layer Normalization. Positional Encoding cho sequence order.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 4: Decoder & Language Modeling",
                 "Decoder vá»›i Masked Self-Attention. Cross-Attention giá»¯a Encoder-Decoder. Autoregressive generation process.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 5: BERT - Bidirectional Understanding",
                 "Pre-training vá»›i Masked Language Modeling. [CLS], [SEP] tokens. Fine-tuning cho downstream tasks: classification, NER, QA.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: GPT - Generative Pre-training",
                 "Decoder-only architecture. Next token prediction. Zero-shot, Few-shot learning. GPT-1 â†’ GPT-2 â†’ GPT-3 â†’ ChatGPT evolution.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 7: Vision Transformer (ViT)",
                 "Transformer cho Computer Vision. Image patches nhÆ° 'words'. Position embeddings cho 2D images. ViT vs CNN comparison.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 8: Scaling Laws & Large Language Models",
                 "Model size, data size, compute scaling. Emergent abilities trong large models. Alignment, RLHF. Responsible AI considerations.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Advanced Projects
        card("Project 1: Text Summarization",
             "Dataset news articles: implement Transformer Encoder-Decoder, ROUGE evaluation, compare vá»›i extractive methods, attention visualization.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        card("Project 2: Question Answering System",
             "Dataset SQuAD-style: fine-tune pre-trained BERT, implement extractive QA, evaluation metrics, deploy vá»›i HuggingFace pipeline.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    # ================== CHÆ¯Æ NG 5 ==================
    st.markdown("---")
    st.header("ğŸš€ ChÆ°Æ¡ng 5: Advanced Deep Learning & Applications")
    st.info("ğŸ¯ Má»¥c tiÃªu: CÃ¡c ká»¹ thuáº­t DL nÃ¢ng cao, GANs, deployment vÃ  xu hÆ°á»›ng AI má»›i nháº¥t")

    with st.expander("Xem chi tiáº¿t:"):
        col1, _, col2 = st.columns([8, 1, 8])

        with col1:
            card("BÃ i 1: Generative Adversarial Networks (GANs)",
                 "Generator vs Discriminator game theory. Vanilla GAN training process. Mode collapse, training instability. Applications: image generation, data augmentation.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 2: Advanced GAN Architectures",
                 "DCGAN cho stable training. StyleGAN cho high-quality faces. Conditional GANs. CycleGAN cho style transfer. Ethical considerations.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 3: Autoencoders & Variational AE",
                 "Encoder-Decoder cho unsupervised learning. Latent space representation. VAE cho probabilistic generation. Applications: anomaly detection, denoising.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        with col2:
            card("BÃ i 4: Model Optimization & Deployment",
                 "Model quantization, pruning, distillation. TensorFlow Lite, ONNX. Edge deployment considerations. Inference optimization techniques.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 5: MLOps cho Deep Learning",
                 "Experiment tracking vá»›i Weights & Biases. Model versioning. GPU training workflows. Docker containers cho reproducibility.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )
            card("BÃ i 6: Emerging Trends & Future",
                 "Multimodal models (CLIP, DALL-E). Reinforcement Learning tá»« Human Feedback. Foundation models. AI Safety vÃ  Alignment challenges.",
                 icon="ğŸ“‘",
                 color='#e8f5e9'
                 )

        # Capstone Project
        card("Capstone Project: End-to-End AI Application",
             "Multimodal project: CNN cho image processing + Transformer cho text analysis + web deployment. VÃ­ dá»¥: AI-powered content moderation system hoáº·c medical diagnosis assistant.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

        # Portfolio Projects
        card("Portfolio Showcase: AI Demos",
             "Táº¡o collection cÃ¡c demo AI: chatbot vá»›i Transformer, image generator vá»›i GAN, style transfer app, object detection system. Deploy lÃªn cloud platforms.",
             icon="ğŸ“‹",
             color='linear-gradient(135deg, #fbc2eb, #a6c1ee)'
             )

    st.markdown("---")
    st.success("ğŸ‘‰ HoÃ n thÃ nh Deep Learning track, báº¡n sáº½ hiá»ƒu sÃ¢u vá» neural networks, lÃ m chá»§ CNN, RNN, Transformer vÃ  cÃ³ thá»ƒ xÃ¢y dá»±ng cÃ¡c á»©ng dá»¥ng AI hiá»‡n Ä‘áº¡i!")